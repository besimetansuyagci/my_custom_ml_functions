{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage of missing values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_percent(df):\n",
    "    missing = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([missing, percent], axis=1, keys=['Missing', 'Percent'])\n",
    "    return missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove columns if its missing percentage is above a given value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_missing_columns(df,percent):\n",
    "    missing_data=missing_percent(df)\n",
    "    remaining_colums=missing_data[missing_data['Percent']<percent]\n",
    "    #print('{} are removed'.format(missing_data[missing_data['Percent']>=percent]))\n",
    "    a = remaining_colums.index\n",
    "    return df.loc[:,a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum percentage of records in a single category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any fields that exceed the specified maximum percent are eliminated\n",
    "def single_value_elimination(df,percent):\n",
    "    modes = df.mode().iloc[0,:]\n",
    "    remaining_columns=[]\n",
    "    for i in range(len(df.columns)):\n",
    "        count = 0\n",
    "        for j in range(len(df)):\n",
    "            if df.iloc[j, i] == modes[i]:\n",
    "                count = count + 1\n",
    "        p = (count / len(df))\n",
    "        if p<=percent: remaining_columns.append(df.mode().iloc[0,:].index[i])\n",
    "    return df[remaining_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate outlier values of a single variable by IQR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_iqr(ys):\n",
    "    quartile_1, quartile_3 = np.percentile(ys, [25, 75])\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "    return np.where((ys > upper_bound) | (ys < lower_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Print # of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier if falls outside of 1.5 times of an interquartile range above the 3rd quartile and below the 1st quartile\n",
    "def print_num_outliers_IQR(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    b=np.where((df > (Q3 + 1.5 * IQR)) | (df < (Q1 - 1.5 * IQR)))\n",
    "    print('Total # of outliers: ', len(b[0]))\n",
    "    print('# of rows containing outliers: ',len(np.unique(b[0])),'(%.2f' %(len(np.unique(b[0]))/len(df)),' of data)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print boundary of outliers and row indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_bounds_outlier_indexes(df):\n",
    "    bounds = pd.DataFrame(columns=['Variable','Lower_Bound','Upper_Bound'])\n",
    "    iqr_row_indexes = []\n",
    "    for column in df.columns:\n",
    "        quartile_1, quartile_3 = np.percentile(df[column], [25, 75])\n",
    "        iqr = quartile_3 - quartile_1\n",
    "        lower_bound = quartile_1 - (iqr * 1.5)\n",
    "        upper_bound = quartile_3 + (iqr * 1.5)    \n",
    "        bounds.loc[column] = [column, lower_bound, upper_bound]\n",
    "        iqr_row_indexes.extend(list(np.where((df[column] > upper_bound) | (df[column] < lower_bound))[0]))\n",
    "    iqr_row_indexes = list((set(iqr_row_indexes)))\n",
    "    iqr_row_indexes.sort(key=None, reverse=False)\n",
    "    return bounds, iqr_row_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance by ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(x,y):\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    model = ExtraTreesClassifier()\n",
    "    y = y.values.ravel()\n",
    "    model.fit(x, y)\n",
    "    return model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature Selection by Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recursive Feature Elimination: Works by recursively removing attributes and building a model on those attributes that remain.\n",
    "#It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most \n",
    "#to predicting the target attribute.\n",
    "def RFE(x,y,num_var): \n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression()\n",
    "    rfe = RFE(model, num_var) # You can see that RFE chose the the top num_var features\n",
    "    #y = np.array(y).reshape(-1,1)\n",
    "    y = y.values.ravel()\n",
    "    fit = rfe.fit(x, y)\n",
    "    features_num=fit.n_features_\n",
    "    r_square=fit.score(x,y)\n",
    "    var_rank=fit.ranking_ # These are marked True in the support_ array and marked with a choice 1 in the ranking_ array.\n",
    "    var_selected=fit.support_ # These are marked True in the support_ array and marked with a choice 1 in the ranking_ array.\n",
    "    return features_num, r_square, var_selected,var_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature Selection by SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_selection (x,y,num_var):\n",
    "    #Statistical tests can be used to select those features that have the strongest relationship with the output variable\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    from sklearn.feature_selection import chi2\n",
    "    test = SelectKBest(score_func=chi2,k=num_var)\n",
    "    fit = test.fit(x,y)\n",
    "    chi_square=fit.scores_\n",
    "    features = fit.transform(x)\n",
    "    #ns_df = pd.DataFrame(chi_square, features, columns=['Feat_names', 'F_Scores'])\n",
    "    selected_cols = x.columns[test.get_support()]\n",
    "    return chi_square, features, selected_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(x,y,x_test,num_var):\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=num_var)\n",
    "    fit = pca.fit(x)                    #PCA için mutlaka scale yapılmalı\n",
    "    ratio=fit.explained_variance_ratio_ #toplam %70-80 civarı olması iyi\n",
    "    component_coef=fit.components_      #componentları oluşturan katsayıları verir\n",
    "    component_df=pca.fit_transform(x,y) #her satır için componentların hesaplanmış halini verir\n",
    "    test_set_component_df=fit.transform(x_test)\n",
    "    return ratio, component_coef, component_df, test_set_component_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis Considering GINI Performance Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Univariate_gini(X, y):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    r_square = []\n",
    "    accuracy = []\n",
    "    gini = []\n",
    "    auc = []\n",
    "    for column in X.columns:\n",
    "        LogReg = LogisticRegression()\n",
    "        variable = np.array(X[column]).reshape(-1, 1)\n",
    "        LogReg.fit(variable, y)\n",
    "        y_pred = LogReg.predict(variable)\n",
    "        y_pred_prob = LogReg.predict_proba(variable)[:, 1]\n",
    "        r_square_value = LogReg.score(np.array(X[column]).reshape(-1, 1), y)\n",
    "        r_square.append(r_square_value)\n",
    "        accuracy_score_value = accuracy_score(y, y_pred)\n",
    "        accuracy.append(accuracy_score_value)\n",
    "        auc_value = roc_auc_score(y, y_pred_prob)\n",
    "        auc.append(auc_value)\n",
    "        gini_value = 2 * auc_value - 1\n",
    "        gini.append(gini_value)\n",
    "    df_gini = pd.DataFrame(data=gini, index=X.columns, columns=[\"Gini Values\"]).sort_values(by=\"Gini Values\")\n",
    "    return df_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Optimal Depth of the Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptDepth_DecTree(x_train,y_train,var):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    score_ls = []     # here I will store the roc auc\n",
    "    score_std_ls = [] # here I will store the standard deviation of the roc_auc\n",
    "    for tree_depth in [1,2,3,4]:\n",
    "        tree_model = DecisionTreeClassifier(max_depth=tree_depth)\n",
    "        scores = cross_val_score(tree_model,x_train.iloc[:,int(var)].to_frame(),       \n",
    "        y_train, cv=3, scoring='roc_auc')   \n",
    "        score_ls.append(np.mean(scores))\n",
    "        score_std_ls.append(np.std(scores))  \n",
    "    temp = pd.concat([pd.Series([1,2,3,4]), pd.Series(score_ls), pd.Series(score_std_ls)], axis=1)\n",
    "    temp.columns = ['depth', 'roc_auc_mean', 'roc_auc_std']   \n",
    "    a=np.where(temp.iloc[:,1]==temp.iloc[:,1].max()) #max auc'nin indexini verir\n",
    "    s = int(a[0])\n",
    "    #print(x_train.iloc[:,int(var)].name)\n",
    "    #print(temp)\n",
    "    return s+1 # gives depth value having max auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorizing variables by using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Binnig_DecTree(x_train,y_train,x_test,opt_depth):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    c=len(x_train.columns)\n",
    "    for j in range(c):       \n",
    "        tree_model=DecisionTreeClassifier(max_depth=opt_depth[j], min_samples_leaf=int(len(x_train)*0.1))\n",
    "        tree_model.fit(x_train.iloc[:,j].to_frame(), y_train)            \n",
    "        x_train['TREE_'+str(x_train.iloc[:,j].name)]=tree_model.predict_proba(x_train.iloc[:,j].to_frame())[:,1] \n",
    "\n",
    "        bins=(pd.concat( [x_train.groupby (['TREE_'+str(x_train.iloc[:,j].name)]) [x_train.iloc [:,j].name].max()], axis=1))\n",
    "        bins=np.sort(bins,0)  \n",
    "        bins=[float(i) for i in bins] \n",
    "        del bins[len(bins)-1] \n",
    "        bins.insert(0,-100000000) \n",
    "        bins.append(100000000)    \n",
    "        labels=[i+2 for i in range(len(bins)-1)]                  \n",
    "        category_train = pd.cut(x_train.iloc [:,j],bins=bins,labels=labels) \n",
    "        category_train = category_train.to_frame()\n",
    "        category_train.columns = ['BIN_'+x_train.iloc[:,j].name]       \n",
    "        x_train = pd.concat([x_train,category_train],axis = 1)\n",
    "                \n",
    "        category_test = pd.cut(x_test.iloc [:,j],bins=bins,labels=labels) \n",
    "        category_test = category_test.to_frame()\n",
    "        category_test.columns = ['BIN_'+x_test.iloc[:,j].name]       \n",
    "        x_test = pd.concat([x_test,category_test],axis = 1)\n",
    "    return x_train,x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation with AUC (Stratified KFold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_skf_cv_auc(x,y,n_splits,classifiers):\n",
    "    import warnings\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    cv = StratifiedKFold(n_splits=n_splits)\n",
    "    df_aucs = pd.DataFrame()\n",
    "\n",
    "    for train_index, test_index in cv.split(x,y):\n",
    "        x_train_skf, x_test_skf = x.iloc[train_index], x.iloc[test_index]\n",
    "        y_train_skf, y_test_skf = y.iloc[train_index], y.iloc[test_index]\n",
    "        aucs = []\n",
    "        for clf in classifiers:\n",
    "            #name=clf.__class__.__name__ \n",
    "            probas_ = clf.fit(x_train_skf, y_train_skf.values.ravel()).predict_proba(x_test_skf)\n",
    "            roc_auc = roc_auc_score(y_test_skf, probas_[:, 1])\n",
    "            aucs.append(roc_auc) \n",
    "    aucs = pd.Series(aucs)\n",
    "    df_aucs=df_aucs.append(aucs,ignore_index=True)\n",
    "    return df_aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation by Train-Test-Initial Test Set AUC (Repeated KFold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rkf_cv_auc(X,y,selected_cols,X_test,y_test,splits,repeats,classifiers):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    rkf = RepeatedKFold(n_splits=splits, n_repeats=repeats, random_state=2652124)\n",
    "    log_cols=[\"Classifier\", \"TRAIN_AUC\", \"TEST_AUC\", \"VAL_AUC\"]\n",
    "    log = pd.DataFrame(columns=log_cols)\n",
    "    sample_id = 0\n",
    "    for train_index, test_index in rkf.split(X):\n",
    "    \n",
    "        X_train_rkf, X_test_rkf = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_rkf, y_test_rkf = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "        sample_id = sample_id + 1\n",
    "        \n",
    "        for clf in classifiers:\n",
    "            \n",
    "            rfe_model = clf.fit(X_train_rkf, y_train_rkf.values.ravel())\n",
    "            name = clf.__class__.__name__\n",
    "    \n",
    "            auc_X_train_rkf = roc_auc_score(y_train_rkf, rfe_model.predict_proba(X_train_rkf)[:,1])\n",
    "            auc_X_test_rkf = roc_auc_score(y_test_rkf, rfe_model.predict_proba(X_test_rkf)[:,1])\n",
    "            auc_X_test = roc_auc_score(y_test, rfe_model.predict_proba(X_test.loc[:,selected_cols])[:,1])\n",
    "    \n",
    "            log = log.append({\"Classifier\": str(sample_id) + '-' + str(name), \"TRAIN_AUC\": auc_X_train_rkf, \"TEST_AUC\": auc_X_test_rkf, \"VAL_AUC\": auc_X_test }, ignore_index=True)\n",
    "    \n",
    "    log[['sample id','model name']] = log.Classifier.str.split(\"-\",expand=True,)\n",
    "    auc_means = log.groupby(['model name'])['TRAIN_AUC','TEST_AUC', 'VAL_AUC'].mean()\n",
    "    auc_stds = log.groupby(['model name'])['TRAIN_AUC','TEST_AUC', 'VAL_AUC'].std()\n",
    "    \n",
    "    for clf in classifiers:\n",
    "        name = clf.__class__.__name__\n",
    "        print(name, \"rkf AUC average: %0.3f (+/- 2std %0.3f)\" % (auc_means.loc[name, 'TEST_AUC'], auc_stds.loc[name, 'TEST_AUC'] * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
